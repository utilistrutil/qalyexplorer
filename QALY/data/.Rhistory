patients_savings_premium = mean(patients_savings_premium, na.rm = TRUE),
net_promoter_score = mean(net_promoter_score, na.rm = TRUE),
pct_employees_trained = mean(pct_employees_trained, na.rm = TRUE),
decent_jobs = mean(decent_jobs[out_decent_jobs==0], na.rm = TRUE),
investee_revenue_growth = mean(investee_revenue_growth[out_investee_revenue_growth==0], na.rm = TRUE)
)
latest_export_hc %>%
summarise(
patients_treated = mean(patients_treated, na.rm = TRUE),
patients_new_access = mean(patients_new_access[out_patients_new_access==0], na.rm = TRUE),
patients_savings_premium = mean(patients_savings_premium, na.rm = TRUE),
net_promoter_score = mean(net_promoter_score, na.rm = TRUE),
pct_employees_trained = mean(pct_employees_trained, na.rm = TRUE),
decent_jobs = mean(decent_jobs[out_decent_jobs==0], na.rm = TRUE),
investee_revenue_growth = mean(investee_revenue_growth[out_investee_revenue_growth==0], na.rm = TRUE)
)
latest_export_hc %>%
filter(asset_class = "Private equity") %>%
summarise(
patients_treated = mean(patients_treated[out_patients_treated==0], na.rm = TRUE),
patients_new_access = mean(patients_new_access[out_patients_new_access==0], na.rm = TRUE),
patients_savings_premium = mean(patients_savings_premium, na.rm = TRUE),
net_promoter_score = mean(net_promoter_score, na.rm = TRUE),
pct_employees_trained = mean(pct_employees_trained, na.rm = TRUE),
decent_jobs = mean(decent_jobs[out_decent_jobs==0], na.rm = TRUE),
investee_revenue_growth = mean(investee_revenue_growth[out_investee_revenue_growth==0], na.rm = TRUE)
)
latest_export_hc %>%
filter(asset_class == "Private equity") %>%
summarise(
patients_treated = mean(patients_treated[out_patients_treated==0], na.rm = TRUE),
patients_new_access = mean(patients_new_access[out_patients_new_access==0], na.rm = TRUE),
patients_savings_premium = mean(patients_savings_premium, na.rm = TRUE),
net_promoter_score = mean(net_promoter_score, na.rm = TRUE),
pct_employees_trained = mean(pct_employees_trained, na.rm = TRUE),
decent_jobs = mean(decent_jobs[out_decent_jobs==0], na.rm = TRUE),
investee_revenue_growth = mean(investee_revenue_growth[out_investee_revenue_growth==0], na.rm = TRUE)
)
library(tidyverse)
library(dplyr)
library(readxl)
library(lubridate)
library(wbstats)
# README ------------------------------------------------------------------
###After setting your username in the "SET" section, this process can and should be run through in one go
###Please ensure you have none of the documents queried (taxonomy, codebook, etc.) opened, as this will return an error
###For the time being, the csv exports into the testing/references folder
# SET ---------------------------------------------------------------------
username <- "JTate"
# Import reference materials ----------------------------------------------
###Define LDCs
###LDC data is used for revenue growth SDG
###There is currently no API for members of the LDCs, so they are defined here
ldcs <-  c("AFG", "AGO", "BGD", "BEN", "BFA", "BDI", "KHM", "CAF", "TCD", "COM", "COD", "DJI",
"ERI", "ETH", "GMB", "GIN", "GNB", "HTI", "KIR", "LAO", "LSO", "LBR", "MDG", "MWI", "MLI",
"MRT", "MOZ", "MMR", "NPL", "NER", "RWA", "STP", "SEN", "SLE", "SLB", "SOM", "SSD", "SDN",
"TLS", "TGO", "TUV", "UGA", "TZA", "YEM", "ZMB")
###Import the GIIN taxonomy, key for left joins
####This just preserves iso3c and regions from the UN
giin_taxonomy_geog <- read_xlsx(paste0("C:/Users/", username,"/Global Impact Investing Network/Document Center - RESEARCH/Policies and Guidelines/Taxonomy/2024 09 GIIN Taxonomy Final (updated governance).xlsx"),
sheet = 4,
skip = 2) %>%
select(`ISO3C Code`, `Sub-region Name`,`Intermediate Region Name`) %>%
rename(iso3c = `ISO3C Code`, subregion = `Sub-region Name`, intermediate = `Intermediate Region Name`) %>%
###not all areas have intermediate details, so they need to be replaced with subregions
mutate(most_specific_region = ifelse(is.na(intermediate), subregion, intermediate)) %>%
###this most specific region matches to the upmap
select(-intermediate, -subregion)
###However, we need to upmap these UN regions like so:
###This is for benchmark usability
benchmark_regions <- read.csv(paste0("C:/Users/", username,"/Global Impact Investing Network/Document Center - RESEARCH/Projects - Performance/Benchmarks/Shared calculations/Benchmark region upmap.csv")) %>%
rename(most_specific_region = `Most.specific.region`, region = `Benchmark.display`)
giin_taxonomy_geog <- giin_taxonomy_geog %>%
left_join(benchmark_regions, by = "most_specific_region")
####This takes ISO3C from World Bank to be used for income group matching
giin_taxonomy_income <- read_xlsx(paste0("C:/Users/", username,"/Global Impact Investing Network/Document Center - RESEARCH/Policies and Guidelines/Taxonomy/2024 09 GIIN Taxonomy Final (updated governance).xlsx"),
sheet = 4,
skip = 2) %>%
select(`Code`, `Income group`) %>%
rename(iso3c = Code, income_group = `Income group`)
###Import the iso3c mapping for prior GIIN taxonomy
###The country list on benchmarks does not match updated taxonomy
###Therefore, they must be mapped with iso3c
iso3c_mapping <- read.csv(paste0("C:/Users/", username,"/Global Impact Investing Network/Document Center - RESEARCH/Admin/Database development/Taxonomy/GIIN Country Taxonomy ISO3 Codes.csv"),
skip =1) %>%
select(primary_country = Country, iso3c = `ISO.3.Letter`)
###Import the SF database, key for SF id
###Should be database responsive in future
sf_database <- read.csv(paste0("C:/Users/", username, "/Global Impact Investing Network/Document Center - RESEARCH (Restricted)/Benchmarks/Database Processes/Testing/References/allsfinput.csv")) %>%
select(org_name = sf_org_name, sf_id = sf_org_id)
###Import the column names from the codebook
####Please note that this assumes that the vector must equal the columns in the import in a quite crude way
rename_vector <- read_xlsx(paste0("C:/Users/", username, "/Global Impact Investing Network/Document Center - RESEARCH/Projects - Performance/Benchmarks/5. Healthcare/Healthcare Codebook.xlsx"),
sheet = "Direct Inputs", skip = 0) %>%
select(varname = `Data collection varname`) %>%
na.omit()%>%
pull(varname)
# Import latest dataset ---------------------------------------------------
folder_path <- paste0("C:/Users/", username, "/Global Impact Investing Network/Document Center - RESEARCH (Restricted)/Benchmarks/Healthcare/Database Creation/Outputs/")
# Get the latest CSV file based on YYMMDD prefix in filename
latest_file <- list.files(folder_path, pattern = "\\.csv$", full.names = TRUE) %>%
{.[which.max(as.Date(substr(basename(.), 1, 6), format = "%y%m%d"))]}
# Load and export the latest data
latest_data_hc <- read.csv(latest_file, header = TRUE)[-1] %>%
##since source is not named in codebook, name it here
setNames(tolower(rename_vector)) %>%
rename(source = 65)
# ETL ---------------------------------------------------------------------
##The ETL is effectively one large edit on the dataset
latest_data_hc <-
latest_data_hc %>%
###Matching SF ID
####Name is extracted from source, matched, then dropped
mutate(source_prefix = str_extract(source, "^[^_]+")) %>%
left_join(sf_database, by = c("source_prefix" = "org_name")) %>%
select(-source_prefix) %>%
###Matching taxonomy
####Includes nec. interim of backmap
left_join(iso3c_mapping, by = "primary_country") %>%
left_join(giin_taxonomy_geog, by = "iso3c") %>%
left_join(giin_taxonomy_income, by ="iso3c") %>%
##Normalization ratio safeguard
####Built in redundancy to get rid of questionable normalization ratios
mutate(normalization_ratio = ifelse(normalization_ratio > 1 | normalization_ratio < 0 | reporting_year < year_investment_made,
NA, normalization_ratio)) %>%
###Mutations
####All proceed as per ETL map
mutate(years_since_investment = reporting_year - year_investment_made,
has_investee_engage = ifelse(is.na(investee_engage), NA, ifelse(grepl("None of these mechanisms were used",investee_engage), 0, 1)),
has_stake_engage = ifelse(is.na(stake_engage), NA, ifelse(grepl("None of these mechanisms were used",stake_engage), 0, 1)),
has_terms = ifelse(is.na(terms), NA, ifelse(grepl("None of these mechanisms were used",terms), 0, 1)),
has_quality_char = ifelse(is.na(quality_assurance) & is.na(certifications_frameworks), 0, 1),
###Add escaoes to commas since they are delimited
strat_goals_hc = str_replace(strat_goals_hc, "Increasing Access to Essential Medicines, Medical Supplies, and Vaccines","Increasing Access to Essential Medicines^, Medical Supplies^, and Vaccines"),
disease_addressed = str_replace(disease_addressed, "Endocrine, nutritional, or metabolic diseases","Endocrine^, nutritional^, or metabolic diseases"),
disease_addressed = str_replace(disease_addressed, "Mental, behavioral, or neurodevelopmental disorders", "Mental^, behavioral^, or neurodevelopmental disorders"),
disease_addressed = str_replace(disease_addressed, "Pregnancy, childbirth or the peurperium","Pregnancy^, childbirth or the peurperium"),
patients_treated_n = patients_treated * normalization_ratio,
patients_treated_women_n = patients_treated_women * normalization_ratio,
patients_treated_women_pct = patients_treated_women/patients_treated,
patients_treated_hist_marg_n = patients_treated_hist_marg * normalization_ratio,
patients_treated_hist_marg_pct = patients_treated_hist_marg/patients_treated,
patients_new_access_n = patients_new_access * normalization_ratio,
decent_jobs_n = decent_jobs * normalization_ratio,
decent_jobs_women_n = decent_jobs_women * normalization_ratio,
decent_jobs_women_pct = decent_jobs_women / decent_jobs,
least_developed_country = ifelse(iso3c %in% ldcs, 1, 0)
)%>%
##Pace
####the dataframe is grouped by three unique IDs so that pace can be determined
arrange(sf_id,investee_name, fund_name,reporting_year) %>%
group_by(sf_id,investee_name, fund_name) %>%
mutate(patients_treated_pace = patients_treated - lag(patients_treated, 1),
patients_treated_pace_pct = patients_treated_pace / lag(patients_treated, 1),
patients_treated_women_pace = patients_treated_women - lag(patients_treated_women, 1),
patients_treated_women_pace_pct = patients_treated_women_pace / lag(patients_treated_women, 1),
patients_treated_hist_marg_pace = patients_treated_hist_marg - lag(patients_treated_hist_marg, 1),
patients_treated_hist_marg_pace_pct = patients_treated_hist_marg_pace / lag(patients_treated_hist_marg, 1),
patients_new_access_pace = patients_new_access - lag(patients_new_access,1 ),
patients_new_access_pace_pct = patients_new_access_pace / lag(patients_new_access,1 ),
decent_jobs_pace = decent_jobs - lag(decent_jobs, 1),
decent_jobs_pace_pct = decent_jobs_pace / lag(decent_jobs, 1),
decent_jobs_women_pace = decent_jobs_women - lag(decent_jobs_women, 1),
decent_jobs_womens_pace_pct = decent_jobs_women_pace / lag(decent_jobs_women, 1)
) %>%
ungroup()
# Outliers ----------------------------------------------------------------
# Specify the variables to check for outliers
##This pulls from the outliers column in the codebook
outliers_vector <- read_xlsx(paste0("C:/Users/", username, "/Global Impact Investing Network/Document Center - RESEARCH/Projects - Performance/Benchmarks/5. Healthcare/Healthcare Codebook.xlsx"),
sheet = "ETL Map", skip = 1) %>%
filter(!is.na(`Outliers calculated`)) %>%
select(varname = `Var name`) %>%
na.omit()%>%
pull(varname) %>%
tolower()
##This updates the outlier vector with _n, _pace, and _pace_pct, as needed
outliers_vector <- outliers_vector %>%
setdiff("investee_revenue_growth") %>%
purrr::map(~ c(.x, paste0(.x, "_n"), paste0(.x, "_pace"), paste0(.x, "_pace_pct"))) %>%
unlist() %>%
union("investee_revenue_growth")
# Remove any potential duplicates
outliers_vector <- unique(outliers_vector) %>%
setdiff(c("decent_jobs_women_pace_pct"))
## This calculates the percentiles and bounds for every variable in outliers_vector
## It is very loose with NA handling in general
###Make sure that the separator is unique ("-") and is not in any varnames
bounds <- latest_data_hc %>%
select(any_of(outliers_vector)) %>%
summarise(across(everything(),
list(Q1 = ~quantile(., 0.25, na.rm = TRUE),
Q3 = ~quantile(., 0.75, na.rm = TRUE),
IQR = ~IQR(., na.rm = TRUE)),
.names = "{col}-{fn}")) %>%
pivot_longer(cols = everything(), names_to = c("variable", "stat"), names_sep = "-") %>%
pivot_wider(names_from = stat, values_from = value) %>%
mutate(
low_bound = ifelse(is.na(Q1) | is.na(IQR), NA, Q1 - 100 * IQR),
high_bound = ifelse(is.na(Q3) | is.na(IQR), NA, Q3 + 100 * IQR)
)
# For each variable in outliers_vector, we apply the bounds and flag outliers
## Outliers are positively determined (NA = 0)
## The value of the figure is then compared to the bound
### Testing note: to test whether outliers can be registered, you can modify the low or high bounds in bound to test - bounds$low_bound <- 10
latest_data_hc_with_outliers <- latest_data_hc %>%
mutate(
across(all_of(outliers_vector),
~ case_when(
is.na(.x) ~ 0,
.x < bounds$low_bound[match(cur_column(), bounds$variable)] |
.x > bounds$high_bound[match(cur_column(), bounds$variable)] ~ 1,  # Outlier condition
TRUE ~ 0
),
.names = "out_{.col}") # Create a new column for outlier flags
)
###Our export
write.csv(latest_data_hc_with_outliers, paste0("C:/Users/", username, "/Global Impact Investing Network/Document Center - RESEARCH (Restricted)/Benchmarks/Database Processes/Testing/References/latest_data_hc_4us_",
format(now(), "%Y%m%d_%H%M%S"),".csv"))
# Preparing Modo file -----------------------------------------------------
###Filtering for Modo
modo_needed <- read_xlsx(paste0("C:/Users/", username, "/Global Impact Investing Network/Document Center - RESEARCH/Projects - Performance/Benchmarks/5. Healthcare/Healthcare Codebook.xlsx"),
sheet = "Modo needed", skip = 0) %>%
select(varname = Varname) %>%
na.omit()%>%
pull(varname) %>%
tolower()
##This prepares the df
###outliers are not in the codebook sheet but should be included
latest_data_hc_4modo <- latest_data_hc_with_outliers %>%
select(any_of(modo_needed), starts_with("out_"))
###This exports the df with a timestamp
write.csv(latest_data_hc_4modo, paste0("C:/Users/", username, "/Global Impact Investing Network/Document Center - RESEARCH (Restricted)/Benchmarks/Database Processes/Testing/References/latest_data_hc_4modo_",
format(now(), "%Y%m%d_%H%M%S"),".csv"))
load("C:/Users/JTate/Global Impact Investing Network/Document Center - RESEARCH/Projects - Performance/Impact Performance Concepts/2023 Fund Level Methodology/Content development/Target-setting/Risk/API pulls/merged_indice_region.RData")
shiny::runApp('app1/ts_rework')
runApp('app1/ts_rework')
runApp('app1/ts_rework')
runApp('app1/ts_rework')
runApp('app1/ts_rework')
runApp('app1/ts_rework')
runApp('app1/ts_rework')
runApp('app1/ts_rework')
runApp('app1/ts_rework')
runApp('app1/ts_rework')
runApp('app1/ts_rework')
runApp('app1/ts_rework')
runApp('app1/ts_rework')
library(tidyverse)
library(dplyr)
library(readxl)
library(lubridate)
library(wbstats)
# README ------------------------------------------------------------------
###After setting your username in the "SET" section, this process can and should be run through in one go
###Please ensure you have none of the documents queried (taxonomy, codebook, etc.) opened, as this will return an error
###For the time being, the csv exports into the testing/references folder
# SET ---------------------------------------------------------------------
username <- "JTate"
# Import reference materials ----------------------------------------------
###Define LDCs
###LDC data is used for revenue growth SDG
###There is currently no API for members of the LDCs, so they are defined here
ldcs <-  c("AFG", "AGO", "BGD", "BEN", "BFA", "BDI", "KHM", "CAF", "TCD", "COM", "COD", "DJI",
"ERI", "ETH", "GMB", "GIN", "GNB", "HTI", "KIR", "LAO", "LSO", "LBR", "MDG", "MWI", "MLI",
"MRT", "MOZ", "MMR", "NPL", "NER", "RWA", "STP", "SEN", "SLE", "SLB", "SOM", "SSD", "SDN",
"TLS", "TGO", "TUV", "UGA", "TZA", "YEM", "ZMB")
###Import the GIIN taxonomy, key for left joins
####This just preserves iso3c and regions from the UN
giin_taxonomy_geog <- read_xlsx(paste0("C:/Users/", username,"/Global Impact Investing Network/Document Center - RESEARCH/Policies and Guidelines/Taxonomy/2024 09 GIIN Taxonomy Final (updated governance).xlsx"),
sheet = 4,
skip = 2) %>%
select(`ISO3C Code`, `Sub-region Name`,`Intermediate Region Name`) %>%
rename(iso3c = `ISO3C Code`, subregion = `Sub-region Name`, intermediate = `Intermediate Region Name`) %>%
###not all areas have intermediate details, so they need to be replaced with subregions
mutate(most_specific_region = ifelse(is.na(intermediate), subregion, intermediate)) %>%
###this most specific region matches to the upmap
select(-intermediate, -subregion)
###However, we need to upmap these UN regions like so:
###This is for benchmark usability
benchmark_regions <- read.csv(paste0("C:/Users/", username,"/Global Impact Investing Network/Document Center - RESEARCH/Projects - Performance/Benchmarks/Shared calculations/Benchmark region upmap.csv")) %>%
rename(most_specific_region = `Most.specific.region`, region = `Benchmark.display`)
giin_taxonomy_geog <- giin_taxonomy_geog %>%
left_join(benchmark_regions, by = "most_specific_region")
####This takes ISO3C from World Bank to be used for income group matching
giin_taxonomy_income <- read_xlsx(paste0("C:/Users/", username,"/Global Impact Investing Network/Document Center - RESEARCH/Policies and Guidelines/Taxonomy/2024 09 GIIN Taxonomy Final (updated governance).xlsx"),
sheet = 4,
skip = 2) %>%
select(`Code`, `Income group`) %>%
rename(iso3c = Code, income_group = `Income group`)
###Import the iso3c mapping for prior GIIN taxonomy
###The country list on benchmarks does not match updated taxonomy
###Therefore, they must be mapped with iso3c
iso3c_mapping <- read.csv(paste0("C:/Users/", username,"/Global Impact Investing Network/Document Center - RESEARCH/Admin/Database development/Taxonomy/GIIN Country Taxonomy ISO3 Codes.csv"),
skip =1) %>%
select(primary_country = Country, iso3c = `ISO.3.Letter`)
###Import the SF database, key for SF id
###Should be database responsive in future
sf_database <- read.csv(paste0("C:/Users/", username, "/Global Impact Investing Network/Document Center - RESEARCH (Restricted)/Benchmarks/Database Processes/Testing/References/allsfinput.csv")) %>%
select(org_name = sf_org_name, sf_id = sf_org_id)
###Import the column names from the codebook
####Please note that this assumes that the vector must equal the columns in the import in a quite crude way
rename_vector <- read_xlsx(paste0("C:/Users/", username, "/Global Impact Investing Network/Document Center - RESEARCH/Projects - Performance/Benchmarks/5. Healthcare/Healthcare Codebook.xlsx"),
sheet = "Direct Inputs", skip = 0) %>%
select(varname = `Data collection varname`) %>%
na.omit()%>%
pull(varname)
# Import latest dataset ---------------------------------------------------
folder_path <- paste0("C:/Users/", username, "/Global Impact Investing Network/Document Center - RESEARCH (Restricted)/Benchmarks/Healthcare/Database Creation/Outputs/")
# Get the latest CSV file based on YYMMDD prefix in filename
latest_file <- list.files(folder_path, pattern = "\\.csv$", full.names = TRUE) %>%
{.[which.max(as.Date(substr(basename(.), 1, 6), format = "%y%m%d"))]}
# Load and export the latest data
latest_data_hc <- read.csv(latest_file, header = TRUE)[-1] %>%
##since source is not named in codebook, name it here
setNames(tolower(rename_vector)) %>%
rename(source = 65)
# ETL ---------------------------------------------------------------------
##The ETL is effectively one large edit on the dataset
latest_data_hc <-
latest_data_hc %>%
###Matching SF ID
####Name is extracted from source, matched, then dropped
mutate(source_prefix = str_extract(source, "^[^_]+")) %>%
left_join(sf_database, by = c("source_prefix" = "org_name")) %>%
select(-source_prefix) %>%
###Matching taxonomy
####Includes nec. interim of backmap
left_join(iso3c_mapping, by = "primary_country") %>%
left_join(giin_taxonomy_geog, by = "iso3c") %>%
left_join(giin_taxonomy_income, by ="iso3c") %>%
##Normalization ratio safeguard
####Built in redundancy to get rid of questionable normalization ratios
mutate(normalization_ratio = ifelse(normalization_ratio > 1 | normalization_ratio < 0 | reporting_year < year_investment_made,
NA, normalization_ratio)) %>%
###Mutations
####All proceed as per ETL map
mutate(years_since_investment = reporting_year - year_investment_made,
has_investee_engage = ifelse(is.na(investee_engage), NA, ifelse(grepl("None of these mechanisms were used",investee_engage), 0, 1)),
has_stake_engage = ifelse(is.na(stake_engage), NA, ifelse(grepl("None of these mechanisms were used",stake_engage), 0, 1)),
has_terms = ifelse(is.na(terms), NA, ifelse(grepl("None of these mechanisms were used",terms), 0, 1)),
has_quality_char = ifelse(is.na(quality_assurance) & is.na(certifications_frameworks), 0, 1),
###Add escaoes to commas since they are delimited
strat_goals_hc = str_replace(strat_goals_hc, "Increasing Access to Essential Medicines, Medical Supplies, and Vaccines","Increasing Access to Essential Medicines^, Medical Supplies and Vaccines"),
disease_addressed = str_replace(disease_addressed, "Endocrine, nutritional, or metabolic diseases","Endocrine^, nutritional or metabolic diseases"),
disease_addressed = str_replace(disease_addressed, "Mental, behavioral, or neurodevelopmental disorders", "Mental^, behavioral or neurodevelopmental disorders"),
disease_addressed = str_replace(disease_addressed, "Pregnancy, childbirth or the peurperium","Pregnancy^, childbirth or the peurperium"),
patients_treated_n = patients_treated * normalization_ratio,
patients_treated_women_n = patients_treated_women * normalization_ratio,
patients_treated_women_pct = patients_treated_women/patients_treated,
patients_treated_hist_marg_n = patients_treated_hist_marg * normalization_ratio,
patients_treated_hist_marg_pct = patients_treated_hist_marg/patients_treated,
patients_new_access_n = patients_new_access * normalization_ratio,
decent_jobs_n = decent_jobs * normalization_ratio,
decent_jobs_women_n = decent_jobs_women * normalization_ratio,
decent_jobs_women_pct = decent_jobs_women / decent_jobs,
least_developed_country = ifelse(iso3c %in% ldcs, 1, 0)
)%>%
##Pace
####the dataframe is grouped by three unique IDs so that pace can be determined
arrange(sf_id,investee_name, fund_name,reporting_year) %>%
group_by(sf_id,investee_name, fund_name) %>%
mutate(patients_treated_pace = patients_treated - lag(patients_treated, 1),
patients_treated_pace_pct = patients_treated_pace / lag(patients_treated, 1),
patients_treated_women_pace = patients_treated_women - lag(patients_treated_women, 1),
patients_treated_women_pace_pct = patients_treated_women_pace / lag(patients_treated_women, 1),
patients_treated_hist_marg_pace = patients_treated_hist_marg - lag(patients_treated_hist_marg, 1),
patients_treated_hist_marg_pace_pct = patients_treated_hist_marg_pace / lag(patients_treated_hist_marg, 1),
patients_new_access_pace = patients_new_access - lag(patients_new_access,1 ),
patients_new_access_pace_pct = patients_new_access_pace / lag(patients_new_access,1 ),
decent_jobs_pace = decent_jobs - lag(decent_jobs, 1),
decent_jobs_pace_pct = decent_jobs_pace / lag(decent_jobs, 1),
decent_jobs_women_pace = decent_jobs_women - lag(decent_jobs_women, 1),
decent_jobs_womens_pace_pct = decent_jobs_women_pace / lag(decent_jobs_women, 1)
) %>%
ungroup()
# Outliers ----------------------------------------------------------------
# Specify the variables to check for outliers
##This pulls from the outliers column in the codebook
outliers_vector <- read_xlsx(paste0("C:/Users/", username, "/Global Impact Investing Network/Document Center - RESEARCH/Projects - Performance/Benchmarks/5. Healthcare/Healthcare Codebook.xlsx"),
sheet = "ETL Map", skip = 1) %>%
filter(!is.na(`Outliers calculated`)) %>%
select(varname = `Var name`) %>%
na.omit()%>%
pull(varname) %>%
tolower()
##This updates the outlier vector with _n, _pace, and _pace_pct, as needed
outliers_vector <- outliers_vector %>%
setdiff("investee_revenue_growth") %>%
purrr::map(~ c(.x, paste0(.x, "_n"), paste0(.x, "_pace"), paste0(.x, "_pace_pct"))) %>%
unlist() %>%
union("investee_revenue_growth")
# Remove any potential duplicates
outliers_vector <- unique(outliers_vector) %>%
setdiff(c("decent_jobs_women_pace_pct"))
## This calculates the percentiles and bounds for every variable in outliers_vector
## It is very loose with NA handling in general
###Make sure that the separator is unique ("-") and is not in any varnames
bounds <- latest_data_hc %>%
select(any_of(outliers_vector)) %>%
summarise(across(everything(),
list(Q1 = ~quantile(., 0.25, na.rm = TRUE),
Q3 = ~quantile(., 0.75, na.rm = TRUE),
IQR = ~IQR(., na.rm = TRUE)),
.names = "{col}-{fn}")) %>%
pivot_longer(cols = everything(), names_to = c("variable", "stat"), names_sep = "-") %>%
pivot_wider(names_from = stat, values_from = value) %>%
mutate(
low_bound = ifelse(is.na(Q1) | is.na(IQR), NA, Q1 - 100 * IQR),
high_bound = ifelse(is.na(Q3) | is.na(IQR), NA, Q3 + 100 * IQR)
)
# For each variable in outliers_vector, we apply the bounds and flag outliers
## Outliers are positively determined (NA = 0)
## The value of the figure is then compared to the bound
### Testing note: to test whether outliers can be registered, you can modify the low or high bounds in bound to test - bounds$low_bound <- 10
latest_data_hc_with_outliers <- latest_data_hc %>%
mutate(
across(all_of(outliers_vector),
~ case_when(
is.na(.x) ~ 0,
.x < bounds$low_bound[match(cur_column(), bounds$variable)] |
.x > bounds$high_bound[match(cur_column(), bounds$variable)] ~ 1,  # Outlier condition
TRUE ~ 0
),
.names = "out_{.col}") # Create a new column for outlier flags
)
###Our export
write.csv(latest_data_hc_with_outliers, paste0("C:/Users/", username, "/Global Impact Investing Network/Document Center - RESEARCH (Restricted)/Benchmarks/Database Processes/Testing/References/latest_data_hc_4us_",
format(now(), "%Y%m%d_%H%M%S"),".csv"))
# Preparing Modo file -----------------------------------------------------
###Filtering for Modo
modo_needed <- read_xlsx(paste0("C:/Users/", username, "/Global Impact Investing Network/Document Center - RESEARCH/Projects - Performance/Benchmarks/5. Healthcare/Healthcare Codebook.xlsx"),
sheet = "Modo needed", skip = 0) %>%
select(varname = Varname) %>%
na.omit()%>%
pull(varname) %>%
tolower()
##This prepares the df
###outliers are not in the codebook sheet but should be included
latest_data_hc_4modo <- latest_data_hc_with_outliers %>%
select(any_of(modo_needed), starts_with("out_"))
###This exports the df with a timestamp
write.csv(latest_data_hc_4modo, paste0("C:/Users/", username, "/Global Impact Investing Network/Document Center - RESEARCH (Restricted)/Benchmarks/Database Processes/Testing/References/latest_data_hc_4modo_",
format(now(), "%Y%m%d_%H%M%S"),".csv"))
install.packages("qualtRics")
library(dplyr)
library(qualtRics)
qualtrics_api_credentials(api_key = "EvUEXNKsBkbbheHGZASKB7SMjEsaMnQuyQxHXlOE",
base_url = "https://iad1.qualtrics.com/API/v3/pathToRequest")
qualtrics_api_credentials(api_key = "EvUEXNKsBkbbheHGZASKB7SMjEsaMnQuyQxHXlOE",
base_url = "https://iad1.qualtrics.com")
qualtrics_api_credentials(api_key = "EvUEXNKsBkbbheHGZASKB7SMjEsaMnQuyQxHXlOE",
base_url = "iad1.qualtrics.com")
qualtrics_api_credentials(api_key = "EvUEXNKsBkbbheHGZASKB7SMjEsaMnQuyQxHXlOE",
base_url = "iad1.qualtrics.com",
install = TRUE)
surveys <- all_surveys()
qualtrics_api_credentials(api_key = "VRhhZUOkEwEssd7E3q4ZrfCySaAjila73YooKUBP3cWMz2H7dga1VilTyg5cj19I",
base_url = "iad1.qualtrics.com",
install = TRUE)
qualtrics_api_credentials(api_key = "VRhhZUOkEwEssd7E3q4ZrfCySaAjila73YooKUBP3cWMz2H7dga1VilTyg5cj19I",
base_url = "iad1.qualtrics.com",
install = TRUE,
overwrite = TRUE)
surveys <- all_surveys()
qualtrics_api_credentials(api_key = "31f73761b27ba0eeaa04143eb1787d2a",
base_url = "iad1.qualtrics.com",
install = TRUE,
overwrite = TRUE)
surveys <- all_surveys()
qualtrics_api_credentials(api_key = "EvUEXNKsBkbbheHGZASKB7SMjEsaMnQuyQxHXlOE",
base_url = "iad1.qualtrics.com",
install = TRUE,
overwrite = TRUE)
surveys <- all_surveys()
qualtrics_api_credentials(api_key = "EvUEXNKsBkbbheHGZASKB7SMjEsaMnQuyQxHXlOE",
base_url = "iad1.qualtrics.com",
install = TRUE,
overwrite = TRUE)
surveys <- all_surveys()
install.packages(c("httr", "jsonlite"))
install.packages(c("httr", "jsonlite"))
library(httr)
library(jsonlite)
install.packages(c("httr", "jsonlite"))
qualtrics_api_credentials(api_key = "af37e2a7-3c75-47cb-86f1-95e4d10d85e0",
base_url = "iad1.qualtrics.com",
install = TRUE,
overwrite = TRUE)
library(dplyr)
library(qualtRics)
qualtrics_api_credentials(api_key = "af37e2a7-3c75-47cb-86f1-95e4d10d85e0",
base_url = "iad1.qualtrics.com",
install = TRUE,
overwrite = TRUE)
surveys <- all_surveys()
library(shiny); runApp('app1/gptest/target_setting/app - JT numerics dev.R')
runApp('app1/gptest/target_setting/app - JT numerics dev.R')
runApp('app1/gptest/target_setting/app - JT numerics dev.R')
runApp('app1/gptest/target_setting/app - JT numerics dev.R')
runApp('app1/gptest/target_setting/app - JT numerics dev.R')
runApp('app1/gptest/target_setting/app - JT numerics dev.R')
runApp('app1/gptest/target_setting/app - JT numerics dev.R')
runApp('app1/gptest/target_setting/app - JT numerics dev.R')
runApp('app1/gptest/target_setting/app - JT numerics dev.R')
setwd("C:/Users/JTate/OneDrive - Global Impact Investing Network/Documents/app1/qaly/qalyexplorer/QALY/data")
load("qalymap.RData")
qalyupdate <- read.csv("C:/Users/JTate/Global Impact Investing Network/Document Center - RESEARCH/Projects - Performance/Impact Performance Concepts/2023 Fund Level Methodology/Content development/QALY tool/updateinfoqalysheet.csv")
qalyupdate <- read.csv("C:/Users/JTate/Global Impact Investing Network/Document Center - RESEARCH/Projects - Performance/Impact Performance Concepts/2023 Fund Level Methodology/Content development/QALY tool/updateinputqalysheet.csv")
updateqalymap$Hover <- qalyupdate$Hover
save(updateqalymap, file = "qalymap.RData")
library(shiny); runApp('~/app1/qaly/qalyexplorer/QALY/apply1.R')
